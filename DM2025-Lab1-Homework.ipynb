{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "GitHub ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Phase Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) that considered as **phase 1 (from exercise 1 to exercise 15)**. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** up **until phase 1**. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    -  Use [the new dataset](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/newdataset/Reddit-stock-sentiment.csv). The dataset contains a 16 columns including 'text' and 'label', with the sentiment labels being: 1.0 is positive, 0.0 is neutral and -1.0 is negative. You can simplify the dataset and use only the columns that you think are necessary. \n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "    - Use this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 10% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    \n",
    "\n",
    "\n",
    "4. Fourth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (September 28th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can keep the answer for phase 1 for easier running and update the phase 2 on the same page.**\n",
    "\n",
    "1. First: Continue doing the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) for **phase 2, starting from Finding frequent patterns**. Use the same master(.ipynb) file. Answer from phase 1 will not be considered at this stage. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: Continue from first phase and do the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** for phase 2, starting from Finding frequent pattern. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    - Continue using this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output. Use the same new dataset as in phase 1.\n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 20% of your grade.__\n",
    "    - Use this file to answer.\n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency).  Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences and when using augmentation with feature pattern.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 19th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\81203\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#環境設定\n",
    "# test code for environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt') # download the NLTK datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import math\n",
    "# If you get \"ModuleNotFoundError: No module named 'PAMI'\"\n",
    "# run the following in a new Jupyter cell:\n",
    "# !pip3 install PAMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         type             datetime  post_id         subreddit  \\\n",
       "0    comment  2025-04-11 17:29:56  mmli62w    wallstreetbets   \n",
       "1    comment   2025-04-12 1:12:19  mmnu7v9    wallstreetbets   \n",
       "2    comment  2025-04-10 15:09:41  mmeevio       StockMarket   \n",
       "3       post  2023-08-30 17:12:55  165kllm  stockstobuytoday   \n",
       "4    comment  2025-04-11 14:48:05  mmkl6bw       StockMarket   \n",
       "..       ...                  ...      ...               ...   \n",
       "842  comment   2021-06-30 4:06:06  h3iv6pq  stockstobuytoday   \n",
       "843  comment   2025-04-11 5:01:24  mmijiuz       StockMarket   \n",
       "844     post  2025-03-24 12:30:39  1jipi4v  stockstobuytoday   \n",
       "845  comment  2025-04-11 20:13:26  mmmely7    wallstreetbets   \n",
       "846  comment   2025-04-12 3:09:06  mmobyz1    wallstreetbets   \n",
       "\n",
       "                                                 title                author  \\\n",
       "0      Retardation is on the menu boys! WSB is so back          StickyTip420   \n",
       "1    Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
       "2    How do you feel about a sitting president maki...          Btankersly66   \n",
       "3                                Who knows more? $VMAR             emiljenfn   \n",
       "4    The Trump administration is begging Xi Jinping...          Just-Big6411   \n",
       "..                                                 ...                   ...   \n",
       "842  $MRIN Marin Software killed it today. Hope som...   Ordinary-Office9180   \n",
       "843                     $ U.S. dollar value (crashing)            lulububudu   \n",
       "844                            Analyst Recommendations               saasfin   \n",
       "845  Weekend Discussion Thread for the Weekend of A...          yes_ur_wrong   \n",
       "846                   Someone post the hotline please.           I_am_Nerman   \n",
       "\n",
       "                                                   url  upvotes  downvotes  \\\n",
       "0                 https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
       "1                 https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
       "2    https://apnews.com/article/trump-truth-social-...        1        NaN   \n",
       "3    https://www.reddit.com/r/stockstobuytoday/comm...       30        0.0   \n",
       "4    https://edition.cnn.com/2025/04/10/politics/tr...        1        NaN   \n",
       "..                                                 ...      ...        ...   \n",
       "842  https://www.reddit.com/r/stockstobuytoday/comm...        1        NaN   \n",
       "843               https://i.redd.it/atvlo83gk4ue1.jpeg        2        NaN   \n",
       "844  https://www.reddit.com/r/stockstobuytoday/comm...        1        0.0   \n",
       "845  https://www.reddit.com/r/wallstreetbets/commen...       10        NaN   \n",
       "846               https://i.redd.it/tcfuu97p7bue1.jpeg        1        NaN   \n",
       "\n",
       "     upvote_ratio                                               text  \\\n",
       "0             NaN                                   Calls on retards   \n",
       "1             NaN  Stunt as in like why did they even make a big ...   \n",
       "2             NaN                  Seeing lots of red in the ticker.   \n",
       "3            0.98  Vision Marine Technologies Inc. is rewriting t...   \n",
       "4             NaN                           He didn’t say thank you.   \n",
       "..            ...                                                ...   \n",
       "842           NaN        Invest now it is going to shoot up tomorrow   \n",
       "843           NaN                             Where can I read this?   \n",
       "844          1.00  Upgrades, Downgrades\\r\\n\\r\\n [Stock Analyst Re...   \n",
       "845           NaN  i sold my puts so i expect cheeto to declare w...   \n",
       "846           NaN                       ![img](emote|t5_2th52|31224)   \n",
       "\n",
       "     subjectivity  polarity  sentiment  \\\n",
       "0        1.000000 -0.900000       -1.0   \n",
       "1        0.177778  0.083333        1.0   \n",
       "2        0.000000  0.000000        0.0   \n",
       "3        0.646970  0.216383        1.0   \n",
       "4        0.000000  0.000000        0.0   \n",
       "..            ...       ...        ...   \n",
       "842      0.000000  0.000000        0.0   \n",
       "843      0.000000  0.000000        0.0   \n",
       "844      0.000000  0.000000        0.0   \n",
       "845      0.000000  0.000000        0.0   \n",
       "846      0.000000  0.000000        0.0   \n",
       "\n",
       "                                              entities  label  \n",
       "0                                                   []   -1.0  \n",
       "1                ['Stunt', 'company', 'deal', 'place']    0.0  \n",
       "2                                           ['ticker']    0.0  \n",
       "3    ['watercraft', 'skill', 'power', ']', 'feat', ...    1.0  \n",
       "4                                                   []   -1.0  \n",
       "..                                                 ...    ...  \n",
       "842                                       ['tomorrow']    1.0  \n",
       "843                                                 []    0.0  \n",
       "844  ['Analyst', 'Nasdaq', 'Stock', 'Upgrades', 'Do...    0.0  \n",
       "845                  ['war', 'denmark', 'cheeto', 'i']   -1.0  \n",
       "846          ['img', ']', '[', 'emote|t5_2th52|31224']    0.0  \n",
       "\n",
       "[847 rows x 16 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#抓資料進來\n",
    "Data_path = r\"C:\\Users\\81203\\DM2025\\DM2025-Lab1-Exercise\\newdataset\\Reddit-stock-sentiment.csv\"\n",
    "df = pd.read_csv(Data_path, encoding=\"utf-8\", engine=\"python\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise, please print out the *text* data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(for example(i+1))\n",
      "Calls on retards\n",
      "(for example(i+1))\n",
      "Stunt as in like why did they even make a big deal about starting it in the first place? No company should ever talk about politics ever.\n",
      "(for example(i+1))\n",
      "Seeing lots of red in the ticker.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"(for example(i+1))\")\n",
    "    print(df.loc[i, \"text\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#先載入helpers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# construct dataframe from a list\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_mining_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdmh\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[43mdmh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m X\n",
      "File \u001b[1;32mc:\\Users\\81203\\DM2025\\DM2025-Lab1-Exercise\\helpers\\data_mining_helpers.py:12\u001b[0m, in \u001b[0;36mformat_rows\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" format the text field and strip special characters \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m D \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m:\n\u001b[0;32m     13\u001b[0m     temp_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(d\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     D\u001b[38;5;241m.\u001b[39mappend([temp_d])\n",
      "File \u001b[1;32mc:\\Users\\81203\\DM2025\\DM2025-Lab1-Exercise\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6318\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6312\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6313\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6314\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6316\u001b[0m ):\n\u001b[0;32m   6317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "#先載入helpers\n",
    "\n",
    "\n",
    "# construct dataframe from a list\n",
    "import helpers.data_mining_helpers as dmh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab1-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
